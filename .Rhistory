min_n = tune(),
tree_depth = tune(),
learn_rate = tune(),
loss_reduction = tune()
) %>%
set_engine("xgboost", objective = "multi:softmax")
# grid specification
xgboost_params <-
dials::parameters(
min_n(),
tree_depth(),
learn_rate(),
loss_reduction()
)
xgboost_grid <-
dials::grid_max_entropy(
xgboost_params,
size = 5
)
head(xgboost_grid)
xgboost_wf <-
workflows::workflow() %>%
add_model(xgboost_model) %>%
add_formula(class ~ .)
# hyperparameter tuning
xgboost_tuned <- tune::tune_grid(
object = xgboost_wf,
resamples = sent_cv_folds,
grid = xgboost_grid,
metrics = yardstick::metric_set(accuracy, kap),
control = tune::control_grid(verbose = TRUE)
)
xgboost_tuned %>% tune::show_best(metric = "accuracy")
xgboost_best_params <- xgboost_tuned %>% tune::select_best("accuracy")
# Plot the results
library(data.table)
xgboost_tuned %>% collect_metrics() %>%
select(mean,min_n:loss_reduction) %>% data.table() %>%
melt(id="mean") %>%
ggplot(aes(y=mean,x=value,colour=variable)) +
geom_point(show.legend = FALSE) +
facet_wrap(variable~. , scales="free") + theme_bw() +
labs(y="Accuracy", x = "Parameter")
xgboost_model <-
parsnip::boost_tree(
mode = "classification",
trees = 5000,
min_n = tune(),
tree_depth = tune(),
learn_rate = tune(),
loss_reduction = tune()
) %>%
set_engine("xgboost")
#, objective = "multi:softmax"
# grid specification
xgboost_params <-
dials::parameters(
min_n(),
tree_depth(),
learn_rate(),
loss_reduction()
)
xgboost_grid <-
dials::grid_max_entropy(
xgboost_params,
size = 5
)
head(xgboost_grid)
xgboost_wf <-
workflows::workflow() %>%
add_model(xgboost_model) %>%
add_formula(class ~ .)
# hyperparameter tuning
xgboost_tuned <- tune::tune_grid(
object = xgboost_wf,
resamples = sent_cv_folds,
grid = xgboost_grid,
metrics = yardstick::metric_set(accuracy, kap),
control = tune::control_grid(verbose = TRUE)
)
xgboost_model <-
parsnip::boost_tree(
mode = "classification",
trees = 500,
min_n = tune(),
tree_depth = tune(),
learn_rate = tune(),
loss_reduction = tune()
) %>%
set_engine("xgboost")
#, objective = "multi:softmax"
# grid specification
xgboost_params <-
dials::parameters(
min_n(),
tree_depth(),
learn_rate(),
loss_reduction()
)
xgboost_grid <-
dials::grid_max_entropy(
xgboost_params,
size = 5
)
head(xgboost_grid)
xgboost_wf <-
workflows::workflow() %>%
add_model(xgboost_model) %>%
add_formula(class ~ .)
# hyperparameter tuning
xgboost_tuned <- tune::tune_grid(
object = xgboost_wf,
resamples = sent_cv_folds,
grid = xgboost_grid,
metrics = yardstick::metric_set(accuracy, kap),
control = tune::control_grid(verbose = TRUE)
)
xgboost_tuned %>% tune::show_best(metric = "accuracy")
xgboost_best_params <- xgboost_tuned %>% tune::select_best("accuracy")
# Plot the results
library(data.table)
xgboost_tuned %>% collect_metrics() %>%
select(mean,min_n:loss_reduction) %>% data.table() %>%
melt(id="mean") %>%
ggplot(aes(y=mean,x=value,colour=variable)) +
geom_point(show.legend = FALSE) +
facet_wrap(variable~. , scales="free") + theme_bw() +
labs(y="Accuracy", x = "Parameter")
xgboost_model_final <- xgboost_model %>% finalize_model(xgboost_best_params)
train_processed <- bake(preprocessing_recipe,  new_data = train_data)
train_prediction <- xgboost_model_final %>%
# fit the model on all the training data
fit(
formula = class ~ .,
data    = train_processed
) %>%
# predict the class for the training data
predict(new_data = train_processed) %>%
bind_cols(train_data)
# Make class a factor
train_prediction$class <- as.factor(train_prediction$class)
xgboost_score_train <- train_prediction %>%
yardstick::metrics(class, .pred_class)
xgboost_score_train
preprocessing_recipe <-
recipes::recipe(class ~ ., data = train_data) %>%
recipes::step_mutate(class = as.factor(class)) %>%
prep()
set.seed(123)
sent_cv_folds <-
recipes::bake(
preprocessing_recipe,
new_data = train_data
) %>%
rsample::vfold_cv(v = 3)
xgboost_model <-
parsnip::boost_tree(
mode = "classification",
trees = 1000,
min_n = tune(),
tree_depth = tune(),
learn_rate = tune(),
loss_reduction = tune()
) %>%
set_engine("xgboost")
# grid specification
xgboost_params <-
dials::parameters(
min_n(),
tree_depth(),
learn_rate(),
loss_reduction()
)
xgboost_grid <-
dials::grid_max_entropy(
xgboost_params,
size = 20
)
head(xgboost_grid)
xgboost_wf <-
workflows::workflow() %>%
add_model(xgboost_model) %>%
add_formula(class ~ .)
# hyperparameter tuning
xgboost_tuned <- tune::tune_grid(
object = xgboost_wf,
resamples = sent_cv_folds,
grid = xgboost_grid,
metrics = yardstick::metric_set(accuracy, kap),
control = tune::control_grid(verbose = TRUE)
)
xgboost_tuned %>% tune::show_best(metric = "accuracy")
xgboost_best_params <- xgboost_tuned %>% tune::select_best("accuracy")
# Plot the results
library(data.table)
xgboost_tuned %>% collect_metrics() %>%
select(mean,min_n:loss_reduction) %>% data.table() %>%
melt(id="mean") %>%
ggplot(aes(y=mean,x=value,colour=variable)) +
geom_point(show.legend = FALSE) +
facet_wrap(variable~. , scales="free") + theme_bw() +
labs(y="Accuracy", x = "Parameter")
xgboost_model_final <- xgboost_model %>% finalize_model(xgboost_best_params)
train_processed <- bake(preprocessing_recipe,  new_data = train_data)
train_prediction <- xgboost_model_final %>%
# fit the model on all the training data
fit(
formula = class ~ .,
data    = train_processed
) %>%
# predict the class for the training data
predict(new_data = train_processed) %>%
bind_cols(train_data)
# Make class a factor
train_prediction$class <- as.factor(train_prediction$class)
xgboost_score_train <- train_prediction %>%
yardstick::metrics(class, .pred_class)
xgboost_score_train
# Confusion matrix
cm<-train_prediction %>% yardstick::conf_mat(truth=class,estimate=.pred_class)
autoplot(cm, type = "heatmap")
train_processed <- bake(preprocessing_recipe,  new_data = test_data)
train_prediction <- xgboost_model_final %>%
# fit the model on all the training data
fit(
formula = class ~ .,
data    = train_processed
) %>%
# predict the class for the training data
predict(new_data = train_processed) %>%
bind_cols(test_data)
# Make class a factor
train_prediction$class <- as.factor(train_prediction$class)
xgboost_score_train <- train_prediction %>%
yardstick::metrics(class, .pred_class)
# Confusion matrix
cm<-train_prediction %>% yardstick::conf_mat(truth=class,estimate=.pred_class)
autoplot(cm, type = "heatmap")
xgboost_score_train
train_processed
train_processed %>% tail
# First evaluat on the training data
train_processed <- bake(preprocessing_recipe,  new_data = train_data)
train_prediction <- xgboost_model_final %>%
# fit the model on all the training data
fit(
formula = class ~ .,
data    = train_processed
) %>%
# predict the class for the training data
predict(new_data = train_processed) %>%
bind_cols(train_data)
# Make class a factor
train_prediction$class <- as.factor(train_prediction$class)
xgboost_score_train <- train_prediction %>%
yardstick::metrics(class, .pred_class)
# Confusion matrix
cm<-train_prediction %>% yardstick::conf_mat(truth=class,estimate=.pred_class)
autoplot(cm, type = "heatmap")
# First evaluat on the training data
train_processed <- bake(preprocessing_recipe,  new_data = train_data)
train_prediction <- xgboost_model_final %>%
# fit the model on all the training data
fit(
formula = class ~ .,
data    = train_processed
) %>%
# predict the class for the training data
predict(new_data = train_processed) %>%
bind_cols(train_data)
# Make class a factor
train_prediction$class <- as.factor(train_prediction$class)
xgboost_score_train <- train_prediction %>%
yardstick::metrics(class, .pred_class)
# And now on the test data
test_processed <-  bake(preprocessing_recipe,  new_data = test_data)
test_prediction <- xgboost_model_final %>%
# fit the model on all the training data
fit(
formula = sale_price ~ .,
data    = train_processed
) %>%
# use the training model fit to predict the test data
predict(new_data = test_processed) %>%
bind_cols(test_data)
# And now on the test data
test_processed <-  bake(preprocessing_recipe,  new_data = test_data)
test_prediction <- xgboost_model_final %>%
# fit the model on all the training data
fit(
formula = class ~ .,
data    = train_processed
) %>%
# use the training model fit to predict the test data
predict(new_data = test_processed) %>%
bind_cols(test_data)
# Make class a factor
test_prediction$class <- as.factor(test_prediction$class)
xgboost_score_test <- test_prediction %>%
yardstick::metrics(class, .pred_class)
xgboost_score_test
# Confusion matrix
cm<-test_prediction %>% yardstick::conf_mat(truth=class,estimate=.pred_class)
autoplot(cm, type = "heatmap")
list_lsm
library(landscapemetrics)
list_lsm
?list_lsm
list_lsm(level="patch")
library(lme4)
>glmer()
?glmer
sent
prediction = raster::predict(sent, model=xgboost_model_final)
xgboost_model_final %>% fit(formula = class~.,data=train_processed) %>% predict(new_data=sent)
sent
xgboost_model_final %>% fit(formula = class~.,data=train_processed) %>% raster::predict(new_data=sent)
raster::as.data.frame(sent)
xgboost_model_final %>% fit(formula = class~.,data=train_processed) %>% raster::predict(new_data=sent %>% raster::as.data.frame())
svm_prediction
ras[1:(nrow(sent)*ncol(sent))]
sent[1:(nrow(sent)*ncol(sent))]
nrow(sent)
ncol(sent)
sent[1:100]
raster::as.array()
raster::as.array(sent)
raster::as.data.frame(sent)
svm_model
# Apply the support vector machines model to the Sentinel-2 data
svm_prediction = raster::predict(sent, model=svm_model)
svm_prediction
sent
svm_model
xgboost_model_final %>%
fit(formula = class~.,data=train_processed) %>%
raster::predict(sent)
xgboost_model_final %>%
fit(formula = class~.,data=train_processed) %>%
predict(sent)
sent
xgboost_model_final %>%
fit(formula = class~.,data=train_processed) %>%
predict(sent %>% raster::as.matrix())
pred <- xgboost_model_final %>%
fit(formula = class~.,data=train_processed) %>%
predict(sent %>% raster::as.data.frame())
pred
sent
matrix(pred,nrow(sent),ncol(sent))
pred
dim(pred) <- c(nrow(sent),ncol(sent))
dim(pred) <- c(nrow(pred),ncol(pred))
pred
nrow(sent)*ncol(sent)
sent
matrix(pred,ncol = 11)
matrix(pred,ncol = 11, byrow = T)
matrix(pred,ncol = 11, byrow = T) %>% as.data.frame()
pred %>% tail()
pred$.pred_class %>% hist()
pred$.pred_class %>% as.numeric() %>% hist()
pred$.pred_class %>% matrix(nrow(sent),ncol(sent)) %>% as.data.frame() -> res
res
pred$.pred_class %>% as.numeric() %>% matrix(nrow(sent),ncol(sent)) %>% as.data.frame() -> res
res
pred <- xgboost_model_final %>%
fit(formula = class~.,data=train_processed) %>%
predict(sent[1:(nrow(sent)*ncol(sent))])
bookdown::render_book("index.Rmd", "bookdown::gitbook")
setwd("~/Documents/GitHub/writeup/Final_project")
setwd("~/Documents/GitHub/writeup/Final_project")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
?knitr::table
?knitr::kable
knitr::kable(head(mtcars[, 1:4]), "pipe")
knitr::kable(head(mtcars[, 1:4]))
knitr::kable(head(mtcars[, 1:4])) -> stg
stg
str(stg)
mtcars[, 1:4]
xgboost_score_test
data.frame("Level 1 Classification" = c(A,B,C,D,E,F,G,H,I,J,K,X))
data.frame("Level 1 Classification" = c("A","B","C","D","E","F","G","H","I","J","K","X"))
data.frame("L1" = c("A","B","C","D","E","F","G","H","I","J","K","X"))
tab < data.frame(class = c("A","B","C","D","E","F","G","H","I","J","K","X"),
desc = c("Marine habitats",
"Coastal habitats",
"Inland surface waters",
"Mires, bogs and fens",
"Grasslands and lands dominated by forbs, mosses or lichens",
"Heathland, scrub and tundra",
"Woodland, forest and other wooded land",
"Inland unvegetated or sparsely vegetated habitats",
"Regularly or recently cultivated agricultural, horticultural and domestic habitats",
"Constructed, industrial and other artificial habitats",
"Montane habitats",
"Habitat complexes"))
tab <- data.frame(class = c("A","B","C","D","E","F","G","H","I","J","K","X"),
desc = c("Marine habitats",
"Coastal habitats",
"Inland surface waters",
"Mires, bogs and fens",
"Grasslands and lands dominated by forbs, mosses or lichens",
"Heathland, scrub and tundra",
"Woodland, forest and other wooded land",
"Inland unvegetated or sparsely vegetated habitats",
"Regularly or recently cultivated agricultural, horticultural and domestic habitats",
"Constructed, industrial and other artificial habitats",
"Montane habitats",
"Habitat complexes"))
tab
tab <- data.frame(class = c("A","B","C","D","E","F","G","H","I","J","K","X"),
desc = c("Marine habitats",
"Coastal habitats",
"Inland surface waters",
"Mires, bogs and fens",
"Grasslands and lands dominated by forbs, mosses or lichens",
"Heathland, scrub and tundra",
"Woodland, forest and other wooded land",
"Inland unvegetated or sparsely vegetated habitats",
"Regularly or recently cultivated agricultural, horticultural and domestic habitats",
"Constructed, industrial and other artificial habitats",
"Montane habitats",
"Habitat complexes"))
knitr::kable(tab,
col.names = c("Level 1 Classification",
"Habitat Description"),
booktabs = TRUE,
caption = "EUNIS habiat classificaiton"
)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
tab <- data.frame(class = c("A","B","C","D","E","F","G","H","I","J","K","X"),
desc = c("Marine habitats",
"Coastal habitats",
"Inland surface waters",
"Mires, bogs and fens",
"Grasslands and lands dominated by forbs, mosses or lichens",
"Heathland, scrub and tundra",
"Woodland, forest and other wooded land",
"Inland unvegetated or sparsely vegetated habitats",
"Regularly or recently cultivated agricultural, horticultural and domestic habitats",
"Constructed, industrial and other artificial habitats",
"Montane habitats",
"Habitat complexes"))
knitr::kable(tab,
col.names = c("L1 Classification",
"Habitat Description"),
booktabs = TRUE,
caption = "EUNIS habiat classificaiton"
)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::html_document2")
bookdown::render_book("index.Rmd", "bookdown::html_document2")
bookdown::render_book("index.Rmd", "bookdown::html_document2")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::html_document2")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
library(knitr)
tab <- data.frame(class = c("A","B","C","D","E","F","G","H","I","J","K","X"),
desc = c("Marine habitats",
"Coastal habitats",
"Inland surface waters",
"Mires, bogs and fens",
"Grasslands and lands dominated by forbs, mosses or lichens",
"Heathland, scrub and tundra",
"Woodland, forest and other wooded land",
"Inland unvegetated or sparsely vegetated habitats",
"Regularly or recently cultivated agricultural, horticultural and domestic habitats",
"Constructed, industrial and other artificial habitats",
"Montane habitats",
"Habitat complexes"))
knitr::kable(
tab,
col.names = c("L1 Classification",
"Habitat Description"),
booktabs = TRUE,
caption = "EUNIS habiat classificaiton"
)
bookdown::render_book("02-methodology.Rmd", "bookdown::html_document2")
bookdown::render_book("02-methodology.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
tab
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::render_book("index.Rmd", "bookdown::gitbook")
